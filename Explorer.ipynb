{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def clone_repo(repo_url: str, dest: str = \"./data\") -> None:\n",
    "    \"\"\"\n",
    "    Clone a Git repository from the given URL into the destination\n",
    "    directory.\n",
    "\n",
    "    Args:\n",
    "        repo_url: HTTPS URL of the Git repository to clone.\n",
    "        dest:    Path to the local directory where the repo will be cloned.\n",
    "    \"\"\"\n",
    "    if not repo_url.startswith((\"http://\", \"https://\")):\n",
    "        raise ValueError(\"The repository URL must start with http:// or https://\")\n",
    "\n",
    "    if not os.path.isdir(dest):\n",
    "        os.makedirs(dest)\n",
    "        print(f\"Created directory: {os.path.abspath(dest)}\")\n",
    "\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\"git\", \"clone\", repo_url, dest],\n",
    "            check=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "        )\n",
    "        print(f\"Repository cloned into {os.path.abspath(dest)}\")\n",
    "    except subprocess.CalledProcessError as exc:\n",
    "        print(f\"Error cloning repository:\\n{exc.stderr.decode().strip()}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository cloned into c:\\Users\\gabin\\OneDrive\\Bureau\\Projets Perso\\RAG_Explorer-for-repository\\data\\project1\n"
     ]
    }
   ],
   "source": [
    "clone_repo(\"https://github.com/pixegami/langchain-rag-tutorial.git\", \"./data/project1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "\n",
    "def load_project_documents(project_path: str):\n",
    "    \"\"\"\n",
    "    Load project files with specific extensions into Document objects.\n",
    "\n",
    "    Args:\n",
    "        project_path: Path to the cloned Git repository.\n",
    "\n",
    "    Returns:\n",
    "        A list of Document objects with metadata preserving file paths.\n",
    "    \"\"\"\n",
    "    glob_patterns = [\n",
    "        \"**/*.py\",    # Python source files\n",
    "        \"**/*.md\",    # Markdown documentation\n",
    "        \"**/*.json\",  # JSON config or data files\n",
    "        \"**/*.yaml\",  # YAML config files\n",
    "        \"**/*.yml\",   # YML config files\n",
    "        \"**/*.sh\",    # Shell scripts\n",
    "        # \"**/*.ipynb\", # jupyter notebook files\n",
    "        \"**/requirements.txt\", # Only the requirements file\n",
    "    ]\n",
    "    try:\n",
    "        loader = DirectoryLoader(\n",
    "            path=project_path,\n",
    "            glob=glob_patterns,\n",
    "            recursive=True,\n",
    "            show_progress=True,\n",
    "            loader_cls=TextLoader,\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        return documents\n",
    "    except Exception as error:\n",
    "        print(f\"Error loading documents from '{project_path}': {error}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "def load_project_documents(project_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Manually load files, preserving all whitespace.\n",
    "    \"\"\"\n",
    "    root = Path(project_path)\n",
    "    if not root.is_dir():\n",
    "        raise FileNotFoundError(f\"{project_path} is not a valid directory.\")\n",
    "\n",
    "    patterns = [\n",
    "        \"*.py\", \"*.md\", \"*.json\", \"*.yaml\",\n",
    "        \"*.yml\", \"*.sh\", \"requirements.txt\",\n",
    "    ]\n",
    "    try:\n",
    "        docs: List[Document] = []\n",
    "        for pat in patterns:\n",
    "            for file in root.rglob(pat):\n",
    "                text = file.read_text(encoding=\"utf-8\")  # keeps indentation\n",
    "                docs.append(Document(page_content=text,\n",
    "                                    metadata={\"source\": str(file)}))\n",
    "                return docs\n",
    "    except Exception as error:\n",
    "        print(f\"Error loading documents from '{project_path}': {error}\")\n",
    "        return []\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: data\\project1\\compare_embeddings.py\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "from langchain.evaluation import load_evaluator\n",
      "from dotenv import load_dotenv\n",
      "import openai\n",
      "import os\n",
      "\n",
      "# Load environment variables. Assumes that project \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\create_database.py\n",
      "# from langchain.document_loaders import DirectoryLoader\n",
      "from langchain_community.document_loaders import DirectoryLoader\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langch \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\query_data.py\n",
      "import argparse\n",
      "# from dataclasses import dataclass\n",
      "from langchain_community.vectorstores import Chroma\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langc \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\README.md\n",
      "# Langchain RAG Tutorial\n",
      "\n",
      "## Install dependencies\n",
      "\n",
      "1. Do the following before installing the dependencies found in `requirements.txt` file because of current challenges installing `onnxruntime` throug \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\data\\books\\alice_in_wonderland.md\n",
      "The Project Gutenberg eBook of Alice's Adventures in Wonderland\n",
      "\n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restr \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\requirements.txt\n",
      "python-dotenv==1.0.1 # For reading environment variables stored in .env file\n",
      "langchain==0.2.2\n",
      "langchain-community==0.2.3\n",
      "langchain-openai==0.1.8 # For embeddings\n",
      "unstructured==0.14.4 # Document loadin \n",
      ".......\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_to_repo = \"./data\"\n",
    "documents = load_project_documents(path_to_repo)\n",
    "for doc in documents:\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(doc.page_content[:200], \"\\n.......\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/rag_explorer/splitter.py\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "splitter.py\n",
    "\n",
    "Split Python code documents into per-function chunks using AST,\n",
    "and leave other docs intact.\n",
    "\"\"\"\n",
    "\n",
    "import ast\n",
    "from typing import List\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def split_code_by_function(doc: Document) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split a Python code document into one Document per function.\n",
    "\n",
    "    Args:\n",
    "        doc: Document whose page_content is Python source code,\n",
    "             and metadata must include \"source\" (file path).\n",
    "\n",
    "    Returns:\n",
    "        A list of Documents, each containing a single function’s code\n",
    "        and metadata[\"function_name\"] set to that function’s name.\n",
    "        If parsing fails or no functions found, returns [doc].\n",
    "    \"\"\"\n",
    "    code = doc.page_content\n",
    "    source_path = doc.metadata.get(\"source\", \"\")\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "    except SyntaxError as e:\n",
    "        print(f\"Return original document cannot be parsed:{e}\")\n",
    "        return [doc]\n",
    "\n",
    "    lines = code.splitlines()\n",
    "    split_docs: List[Document] = []\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            start = node.lineno - 1\n",
    "            # Use end_lineno if available (Python 3.8+), else last body line\n",
    "            end = getattr(node, \"end_lineno\", node.body[-1].lineno)\n",
    "            func_lines = lines[start:end]\n",
    "            func_code = \"\\n\".join(func_lines)\n",
    "            metadata = doc.metadata.copy()\n",
    "            metadata[\"function_name\"] = node.name\n",
    "            split_docs.append(Document(page_content=func_code, metadata=metadata))\n",
    "\n",
    "    # If no functions found, keep the original doc\n",
    "    return split_docs or [doc]\n",
    "\n",
    "\n",
    "def split_documents_by_function(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Apply split_code_by_function to all Python docs, leave others as-is.\n",
    "\n",
    "    Args:\n",
    "        docs: List of Document objects.\n",
    "\n",
    "    Returns:\n",
    "        Expanded list of Document objects, split per Python function.\n",
    "    \"\"\"\n",
    "    result: List[Document] = []\n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get(\"source\", \"\")\n",
    "        if source.endswith(\".py\"):\n",
    "            result.extend(split_code_by_function(doc))\n",
    "        else:\n",
    "            result.append(doc)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: data\\project1\\compare_embeddings.py\n",
      "Function_name: main\n",
      "def main():\n",
      "    # Get embedding for a word.\n",
      "    embedding_function = OpenAIEmbeddings()\n",
      "    vector = embedding_function.embed_query(\"apple\")\n",
      "    print(f\"Vector for 'apple': {vector}\")\n",
      "    print(f\"Vect \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\create_database.py\n",
      "Function_name: main\n",
      "def main():\n",
      "    generate_data_store() \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\create_database.py\n",
      "Function_name: generate_data_store\n",
      "def generate_data_store():\n",
      "    documents = load_documents()\n",
      "    chunks = split_text(documents)\n",
      "    save_to_chroma(chunks) \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\create_database.py\n",
      "Function_name: load_documents\n",
      "def load_documents():\n",
      "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
      "    documents = loader.load()\n",
      "    return documents \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\create_database.py\n",
      "Function_name: split_text\n",
      "def split_text(documents: list[Document]):\n",
      "    text_splitter = RecursiveCharacterTextSplitter(\n",
      "        chunk_size=300,\n",
      "        chunk_overlap=100,\n",
      "        length_function=len,\n",
      "        add_start_index=T \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\create_database.py\n",
      "Function_name: save_to_chroma\n",
      "def save_to_chroma(chunks: list[Document]):\n",
      "    # Clear out the database first.\n",
      "    if os.path.exists(CHROMA_PATH):\n",
      "        shutil.rmtree(CHROMA_PATH)\n",
      "\n",
      "    # Create a new DB from the documents.\n",
      "    db \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\query_data.py\n",
      "Function_name: main\n",
      "def main():\n",
      "    # Create CLI.\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\"query_text\", type=str, help=\"The query text.\")\n",
      "    args = parser.parse_args()\n",
      "    query_text = args.query_ \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\README.md\n",
      "no function_name metadata.\n",
      "# Langchain RAG Tutorial\n",
      "\n",
      "## Install dependencies\n",
      "\n",
      "1. Do the following before installing the dependencies found in `requirements.txt` file because of current challenges installing `onnxruntime` throug \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\data\\books\\alice_in_wonderland.md\n",
      "no function_name metadata.\n",
      "The Project Gutenberg eBook of Alice's Adventures in Wonderland\n",
      "\n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restr \n",
      ".......\n",
      "\n",
      "Source: data\\project1\\requirements.txt\n",
      "no function_name metadata.\n",
      "python-dotenv==1.0.1 # For reading environment variables stored in .env file\n",
      "langchain==0.2.2\n",
      "langchain-community==0.2.3\n",
      "langchain-openai==0.1.8 # For embeddings\n",
      "unstructured==0.14.4 # Document loadin \n",
      ".......\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Documents_splitted = split_documents_by_function(documents)\n",
    "for doc in Documents_splitted:\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    try:\n",
    "        print(f\"Function_name: {doc.metadata['function_name']}\")\n",
    "    except:\n",
    "        print(\"no function_name metadata.\")\n",
    "    print(doc.page_content[:200], \"\\n.......\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/rag_explorer/embedder.py\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "embedder.py\n",
    "\n",
    "Wraps OpenAI embedding model using LangChain to transform\n",
    "documents into vector representations.\n",
    "\"\"\"\n",
    "import os \n",
    "from typing import List\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "class OpenAIEmbedder:\n",
    "    \"\"\"\n",
    "    Wrapper for the OpenAI Embedding model using LangChain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str = \"text-embedding-3-large\") -> None:\n",
    "        \"\"\"\n",
    "        Initialize the embedding model.\n",
    "\n",
    "        Args:\n",
    "            model: OpenAI embedding model name.\n",
    "        \"\"\"\n",
    "        self.embedder = OpenAIEmbeddings(model=model)\n",
    "\n",
    "    def embed_documents(self, documents: List[Document]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Compute embeddings for a list of LangChain Documents.\n",
    "\n",
    "        Args:\n",
    "            documents: List of Document objects.\n",
    "\n",
    "        Returns:\n",
    "            A list of vector embeddings.\n",
    "        \"\"\"\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        return self.embedder.embed_documents(texts)\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Embed a single query string.\n",
    "\n",
    "        Args:\n",
    "            query: The input query text.\n",
    "\n",
    "        Returns:\n",
    "            A single vector embedding.\n",
    "        \"\"\"\n",
    "        return self.embedder.embed_query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedder = OpenAIEmbedder()\n",
    "embedding = embedder.embed_query(\"What is the function of this script?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
